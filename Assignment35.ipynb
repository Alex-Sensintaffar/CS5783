{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMomFAgyzytE4hBwwcrBcOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Sensintaffar/CS5783/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 3\n",
        "\n",
        "All data was recorded in file $\\underline{Assignment\\space{} 3\\space{} data.xlsx}$"
      ],
      "metadata": {
        "id": "2RTpLSIrEm42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorboard\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
        "train_images = train_images / 255.0"
      ],
      "metadata": {
        "id": "NY1XJ_VvXohe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1_1"
      ],
      "metadata": {
        "id": "7pcXwfYnZS_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPWr6ICIXh-0"
      },
      "outputs": [],
      "source": [
        "# x_train = train_images.reshape(-1, 28, 28, 1) #add an additional dimension to represent the single-channel\n",
        "# x_test = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "learning_rates = [.1, .01, .005, .001]\n",
        "batch_sizes = [64, 128, 256, 516]\n",
        "losses = []\n",
        "accuracy = []\n",
        "count = 0\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(len(learning_rates)):\n",
        "    for k in range(len(batch_sizes)):\n",
        "      model = tf.keras.models.Sequential()\n",
        "      model.add(tf.keras.layers.Conv2D(2, (3,3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
        "      model.add(tf.keras.layers.Conv2D(4, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(8, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(24, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(40, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(48, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(56, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "\n",
        "      model.add(tf.keras.layers.Flatten())\n",
        "      model.add(tf.keras.layers.Dense(256))\n",
        "      model.add(tf.keras.layers.Activation('relu'))\n",
        "      model.add(tf.keras.layers.Dense(10))\n",
        "      model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "      batch_size = batch_sizes[k]\n",
        "\n",
        "      if i == 0:\n",
        "        opt = keras.optimizers.Adam(learning_rate=learning_rates[j])\n",
        "      elif i == 1:\n",
        "        opt = keras.optimizers.SGD(learning_rate=learning_rates[j])\n",
        "      elif i == 2:\n",
        "        opt = keras.optimizers.RMSprop(learning_rate=learning_rates[j])\n",
        "\n",
        "      model.compile(\n",
        "          optimizer=opt,\n",
        "          loss='sparse_categorical_crossentropy',\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "      model.build(input_shape=(batch_size,28,28,1))\n",
        "      # model.summary()\n",
        "\n",
        "      logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "      tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "      model1 = model\n",
        "\n",
        "      # Train\n",
        "      model1.fit(\n",
        "          train_images,\n",
        "          train_labels, \n",
        "          batch_size=batch_size,\n",
        "          epochs=5, \n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "      # Evaluate\n",
        "      score = model1.evaluate(test_images, test_labels)\n",
        "      losses.append(score[0])\n",
        "      accuracy.append(score[1])\n",
        "      # print('Test loss:', score[0])\n",
        "      # print('Test accuracy:', score[1])\n",
        "      count += 1\n",
        "      print(count)\n",
        "print(losses)\n",
        "print(accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1_2"
      ],
      "metadata": {
        "id": "kpN2D0ribsXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train = train_images.reshape(-1, 28, 28, 1) #add an additional dimension to represent the single-channel\n",
        "# x_test = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "learning_rates = [.1, .01, .005, .001]\n",
        "batch_sizes = [64, 128, 256, 516]\n",
        "losses = []\n",
        "accuracy = []\n",
        "count = 0\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(len(learning_rates)):\n",
        "    for k in range(len(batch_sizes)):\n",
        "      model = tf.keras.models.Sequential()\n",
        "      model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
        "      model.add(tf.keras.layers.Conv2D(56, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(48, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(40, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(24, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(8, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(4, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(2, (3,3), padding='same', activation='relu'))\n",
        "\n",
        "      model.add(tf.keras.layers.Flatten())\n",
        "      model.add(tf.keras.layers.Dense(256))\n",
        "      model.add(tf.keras.layers.Activation('relu'))\n",
        "      model.add(tf.keras.layers.Dense(10))\n",
        "      model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "      batch_size = batch_sizes[k]\n",
        "\n",
        "      if i == 0:\n",
        "        opt = keras.optimizers.Adam(learning_rate=learning_rates[j])\n",
        "      elif i == 1:\n",
        "        opt = keras.optimizers.SGD(learning_rate=learning_rates[j])\n",
        "      elif i == 2:\n",
        "        opt = keras.optimizers.RMSprop(learning_rate=learning_rates[j])\n",
        "\n",
        "      model.compile(\n",
        "          optimizer=opt,\n",
        "          loss='sparse_categorical_crossentropy',\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "      model.build(input_shape=(batch_size,28,28,1))\n",
        "      # model.summary()\n",
        "\n",
        "      logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "      tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "      model1 = model\n",
        "\n",
        "      # Train\n",
        "      model1.fit(\n",
        "          train_images,\n",
        "          train_labels, \n",
        "          batch_size=batch_size,\n",
        "          epochs=5, \n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "      # Evaluate\n",
        "      score = model1.evaluate(test_images, test_labels)\n",
        "      losses.append(score[0])\n",
        "      accuracy.append(score[1])\n",
        "      count += 1\n",
        "      print(count)\n",
        "      # print('Test loss:', score[0])\n",
        "      # print('Test accuracy:', score[1])\n",
        "print(losses)\n",
        "print(accuracy)\n"
      ],
      "metadata": {
        "id": "M8GPW7awZf7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1_3"
      ],
      "metadata": {
        "id": "Azo0FiPqbwIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train = train_images.reshape(-1, 28, 28, 1) #add an additional dimension to represent the single-channel\n",
        "# x_test = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "learning_rates = [.1, .01, .005, .001]\n",
        "batch_sizes = [64, 128, 256, 516]\n",
        "losses = []\n",
        "accuracy = []\n",
        "count = 0\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(len(learning_rates)):\n",
        "    for k in range(len(batch_sizes)):\n",
        "      model = tf.keras.models.Sequential()\n",
        "      model.add(tf.keras.layers.Conv2D(2, (3,3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
        "      model.add(tf.keras.layers.Conv2D(4, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(24, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(8, (3,3), padding='same', activation='relu'))\n",
        "      # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "      model.add(tf.keras.layers.Conv2D(4, (3,3), padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.Conv2D(2, (3,3), padding='same', activation='relu'))\n",
        "\n",
        "      model.add(tf.keras.layers.Flatten())\n",
        "      model.add(tf.keras.layers.Dense(256))\n",
        "      model.add(tf.keras.layers.Activation('relu'))\n",
        "      model.add(tf.keras.layers.Dense(10))\n",
        "      model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "      batch_size = batch_sizes[k]\n",
        "\n",
        "      if i == 0:\n",
        "        opt = keras.optimizers.Adam(learning_rate=learning_rates[j])\n",
        "      elif i == 1:\n",
        "        opt = keras.optimizers.SGD(learning_rate=learning_rates[j])\n",
        "      elif i == 2:\n",
        "        opt = keras.optimizers.RMSprop(learning_rate=learning_rates[j])\n",
        "\n",
        "      model.compile(\n",
        "          optimizer=opt,\n",
        "          loss='sparse_categorical_crossentropy',\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "      model.build(input_shape=(batch_size,28,28,1))\n",
        "      # model.summary()\n",
        "\n",
        "      logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "      tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "      model1 = model\n",
        "\n",
        "      # Train\n",
        "      model1.fit(\n",
        "          train_images,\n",
        "          train_labels, \n",
        "          batch_size=batch_size,\n",
        "          epochs=5, \n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "      # Evaluate\n",
        "      score = model1.evaluate(test_images, test_labels)\n",
        "      losses.append(score[0])\n",
        "      accuracy.append(score[1])\n",
        "      # print('Test loss:', score[0])\n",
        "      # print('Test accuracy:', score[1])\n",
        "      count += 1\n",
        "      print(count)\n",
        "print(losses)\n",
        "print(accuracy)\n"
      ],
      "metadata": {
        "id": "OTujOVI7bxpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 Report)\n",
        "\n",
        "For each of the three CNN used 3 optimizers (Adam, SGD, RMSprop), 4 learning rates (0.1, 0.01, 0.005, 0.001), and 4 batches sizes (64, 128, 256, 516). \n",
        "\n",
        "The first CNN's best accuracy at 98.89% was found using the RMSprop optimizer, 0.001 learning rate, and 256 batch size. The second CNN's best accuracy at 98.95% was found using the Adam optimizer, 0.001 learning rate, and 128 batch size. The third CNN's best accuracy at 98.60% was found using the Adam optimizer, 0.005 learning rate, and 256 batch size.\n",
        "\n",
        "All three CNN had a recorded accuracy above 98% which by NN standards is very good. The CNN preformed well with these hyperparameters becasue they we able to find detailed internal parameters with the lower learning rates and higher batch sizes. The larger batch sizes allowd for more time between updating the weigths while the learning rates were able to take the time needed to find the best weigths. Looking at the top 4 sets of hyperparamters for each of the three CNN it was clear there were certain hyperparameters that would result in the high accuracies. The best hyperparameters were Adam and RMSprop for optimizer, 0.005 and 0.001 learning rates, and 128 and 256 for batch size.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9JG9QjAy5PHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2_1 to 2_3"
      ],
      "metadata": {
        "id": "SXuOtUywjZDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n"
      ],
      "metadata": {
        "id": "qhZCIVut5W1C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [.05, .01, .005, .001, .0005, .0001]\n",
        "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
        "batch_size = 32\n",
        "losses = []\n",
        "accuracies = []\n",
        "count = 0\n",
        "\n",
        "for i in range(len(learning_rates)):\n",
        "  for j in range(len(batch_sizes)):\n",
        "# for i in range(1):\n",
        "#   for j in range(1):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(6, (5, 5), padding='same', strides=(1, 1), activation='relu', input_shape=(32,32,3)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(tf.keras.layers.Conv2D(16, (5, 5), padding='same', strides=(1, 1), activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(tf.keras.layers.Conv2D(120, (5, 5), padding='same', activation='relu'))\n",
        "    \n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(84))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "    opt = keras.optimizers.Adam(learning_rate=learning_rates[i])\n",
        "    batch_size = batch_sizes[j]\n",
        "    # opt = keras.optimizers.Adam(learning_rate=.0001)\n",
        "    # batch_size = 32\n",
        "\n",
        "    model.compile(\n",
        "            optimizer= opt,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "    model.build(input_shape=(batch_size,32,32,3))\n",
        "    # model.summary()\n",
        "    # Define the Keras TensorBoard callback.\n",
        "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "    model1 = model\n",
        "    # model1 = model\n",
        "\n",
        "    # Train the model.\n",
        "\n",
        "    model1.fit(\n",
        "        train_images,\n",
        "        train_labels, \n",
        "        batch_size=batch_size,\n",
        "        epochs=25, \n",
        "        callbacks=[tensorboard_callback])\n",
        "    \n",
        "    score = model1.evaluate(test_images, test_labels)\n",
        "\n",
        "    losses.append(score[0])\n",
        "    accuracies.append(score[1])\n",
        "    count += 1\n",
        "    print(count)\n",
        "\n",
        "\n",
        "print(losses)\n",
        "print(accuracies)"
      ],
      "metadata": {
        "id": "H4_KZfs9jYcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2_1:\n",
        "\n",
        "Using a lower learing rate, around .0001 to .0005, produceed the best results. As the learning rate decreased the accuracy drasticaly increased. When using a learning rate from 0.05 and 0.01 the CNN could barely get above 10% accuracy. The lower accuracies could start to reach above 50% accuracies when tested.\n",
        "\n",
        "Question 2_2:\n",
        "\n",
        "Using a lower batch size, around 32, produced the best accuracies. While the best accuracies were found with batch sizes of 32 there was only a 6% difference between the smallest test values (32) and largest tested values (1024). It is clear the smaller batch sizes are better but not by much.\n",
        "\n",
        "Question 2_3:\n",
        "\n",
        "I tested the CNN with 6 learning rates (.05, .01, .005, .001, .0005, .0001) and 6 different batch sizes (32, 64, 128, 256, 512, 1024). The best preformance was 57.7% with a learning rate of 0.0001 and batch size of 32. The second best set of hyperparameters was 57.37% with a learning rate of 0.0005 and batch size 32."
      ],
      "metadata": {
        "id": "ETdzAU16EScL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2_4"
      ],
      "metadata": {
        "id": "yaNpwU6wsc68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [.05, .01, .005, .001, .0005, .0001]\n",
        "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
        "batch_size = 32\n",
        "losses = []\n",
        "accuracies = []\n",
        "count = 0\n",
        "\n",
        "for i in range(len(learning_rates)):\n",
        "  for j in range(len(batch_sizes)):\n",
        "    input = keras.layers.Input(shape=(32, 32, 3))\n",
        "    flatten = keras.layers.Flatten()(input)\n",
        "    hidden1 = keras.layers.Dense(32, activation='relu')(flatten)\n",
        "    hidden2 = keras.layers.Dense(64, activation='relu')(hidden1)\n",
        "    hidden3 = keras.layers.Dense(128, activation='relu')(hidden2)\n",
        "    hidden4 = keras.layers.Dense(256, activation='relu')(hidden3)\n",
        "    hidden5 = keras.layers.Dense(256, activation='relu')(hidden4)\n",
        "    output = keras.layers.Dense(10, activation='softmax')(hidden5)\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "    opt = keras.optimizers.RMSprop(learning_rate=learning_rates[i])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer= opt,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "    model1 = model\n",
        "    model1.summary()\n",
        "    model1.fit(\n",
        "          train_images,\n",
        "          train_labels, \n",
        "          batch_size=batch_sizes[j],\n",
        "          epochs=5, \n",
        "          callbacks=[tensorboard_callback])\n",
        "    \n",
        "    score = model1.evaluate(test_images, test_labels)\n",
        "    losses.append(score[0])\n",
        "    accuracies.append(score[1])\n",
        "    count += 1\n",
        "    print(count)\n",
        "    \n",
        "print(losses)\n",
        "print(accuracies)"
      ],
      "metadata": {
        "id": "PbWmpugxselH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2_4_a:\n",
        "\n",
        "The feed forward network had significantly lower preformance than the LeNet CNN. The feed forward's best accruacy was 26.51% with hyperparameters 0.0001 learning rate and 128 batch size.\n",
        "\n",
        "Question 2_4_b:\n",
        "\n",
        "The LeNet has 697,046 parameters while the feed forward CNN has 210,154 parameters. Since the LeNet has over twice the accuracy compared to the feed forward, I think it makes the extra parameters worth it."
      ],
      "metadata": {
        "id": "qNEgc5xRLYUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3_1:\n",
        "\n",
        "input dimensions: 6x6\n",
        "filter dimensions: 3x3\n",
        "kernal parameters: 9\n",
        "\n"
      ],
      "metadata": {
        "id": "JRaiwneATB7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3_2"
      ],
      "metadata": {
        "id": "OthNk-9kT6Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "inputMap = np.array([\n",
        "                    [7,5,0,0,3,2],\n",
        "                    [6,4,5,1,4,8],\n",
        "                    [9,0,2,2,5,4],\n",
        "                    [6,3,4,7,9,8],\n",
        "                    [5,7,5,6,9,0],\n",
        "                    [7,9,0,8,2,3]])\n",
        "\n",
        "filterMap= np.array([\n",
        "                    [1,0,-1],\n",
        "                    [2,0,-2],\n",
        "                    [1,0,-1]])\n",
        "\n",
        "secondFilterMap = filterMap.flatten()\n",
        "\n",
        "outputArray = []\n",
        "\n",
        "for i in range(len(inputMap)):\n",
        "    if i <= len(inputMap) - len(filterMap):\n",
        "        for j in range(len(inputMap)):\n",
        "            if j <= len(inputMap) - len(filterMap):\n",
        "                subMap = inputMap[i:i+len(filterMap),j:j+len(filterMap)]\n",
        "                subMap = subMap.flatten()\n",
        "                # print(subMap)\n",
        "                output = np.convolve(secondFilterMap, subMap, 'valid')\n",
        "                outputArray.append(output)\n",
        "outMatrix = np.array(outputArray).reshape(4,4)\n",
        "print(outMatrix)\n"
      ],
      "metadata": {
        "id": "sksDxv0rTyff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3_2:\n",
        "\n",
        "output:\n",
        "$\\begin{bmatrix}\n",
        "-16 & -9 & 4 & 18\\\\\n",
        "-17 & 5 & 10 & 12\\\\\n",
        "-11 & 9 & 17 & -2\\\\\n",
        "-9 & 1 & 15 & -16\n",
        "\\end{bmatrix}$\n",
        "\n",
        "To find the output activation map I created a code that would take the convolution of the filter and a submatrix of the input matrix X. The output convolution value was then added to a list. The submatrix was created by taking a subset from the input matrix of equal dimensions to the filter. This process was repteated for every possible submatrix in the input matrix. The output convolution list was then reshaped into a 4x4 matrix"
      ],
      "metadata": {
        "id": "MDfhESwGUAMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3_3"
      ],
      "metadata": {
        "id": "hnngDmfDW_MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poolWidth = 2\n",
        "poolHeight = 2\n",
        "poolOutputArray = []\n",
        "\n",
        "for i in range(0,len(outMatrix),poolWidth):\n",
        "  for j in range(0, len(outMatrix),poolHeight):\n",
        "    subMap = outMatrix[i:i+poolWidth, j:j+poolHeight]\n",
        "    poolOutputArray.append(np.amax(subMap))\n",
        "\n",
        "poolOutMatrix = np.array(poolOutputArray).reshape(poolWidth, poolHeight)\n",
        "print(poolOutMatrix)\n"
      ],
      "metadata": {
        "id": "Dw5cgzUYT_yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3_3:\n",
        "\n",
        "Output:\n",
        "$\\begin{bmatrix}\n",
        "5 & 18\\\\\n",
        "9 & 17\n",
        " \\end{bmatrix}$\n",
        "\n",
        " To find the max pooling of the output matrix from question 3_2, I divided the matrix into 4 equal submatrices. The max value in each submatrix was found then added to a list. The output list was then turned into a 2x2 matrix."
      ],
      "metadata": {
        "id": "aKxqkz-UXBN9"
      }
    }
  ]
}
